# Multimodal Imageâ€“Text Semantic Retrieval System (Zero-Shot)

## ğŸ“Œ Project Overview
This project implements a **Multimodal Semantic Retrieval System** that supports **Text and Image** inputs for semantic search.

Using a pretrained **CLIP (Contrastive Languageâ€“Image Pretraining)** model, the system converts both text and images into a shared embedding space and retrieves the most semantically relevant results using **cosine similarity**.

The system works in a **zero-shot setting**, meaning no additional training or fine-tuning is required.

---

## ğŸ¯ Key Features

### ğŸ”¹ Input Modalities
- ğŸ“ **Text Input** â€“ Natural language queries
- ğŸ–¼ï¸ **Image Input** â€“ Image-based similarity search

### ğŸ”¹ Hybrid Queries
- Text-only search
- Image-only search
- **Text + Image combined query**

### ğŸ”¹ Retrieval Capabilities
- Text â†’ Image retrieval
- Image â†’ Image similarity search
- Image â†’ Text matching
- Zero-shot semantic search
- Top-K ranked results

### ğŸ”¹ System Features
- Cosine similarityâ€“based matching
- Automatic modality detection
- Explainable similarity scores
- Confidence-aware retrieval

---

## ğŸ·ï¸ Domain
- Multimodal Artificial Intelligence  
- Computer Vision  
- Natural Language Processing (NLP)  
- Information Retrieval  
- Semantic Search Systems  

---

## ğŸ§  How the System Works

1. **Text Encoding**  
   - User text queries are encoded using CLIPâ€™s text encoder.

2. **Image Encoding**  
   - Images are encoded using CLIPâ€™s image encoder.

3. **Shared Embedding Space**  
   - Both text and image embeddings lie in the same vector space.

4. **Similarity Matching**  
   - Cosine similarity is used to compare embeddings.
   - The system retrieves the **Top-K most semantically similar results**.

---

## ğŸ” Supported Search Types

| Query Type | Supported |
|-----------|----------|
| Text â†’ Image | âœ… |
| Image â†’ Image | âœ… |
| Image â†’ Text | âœ… |
| Text + Image (Hybrid) | âœ… |
| Zero-shot Inference | âœ… |

---

## âš ï¸ Limitations
- Performs best on **general real-world images**
- Limited performance on:
  - Medical images (X-rays, MRI)
  - Satellite imagery
  - Technical diagrams
- Accuracy depends on image quality and clarity

---

## ğŸ› ï¸ Tech Stack
- **Programming Language:** Python  
- **Model:** CLIP (Pretrained)  
- **Libraries:** PyTorch, NumPy  
- **Similarity Metric:** Cosine Similarity  
- **Frontend (Optional):** Streamlit / Flask  
- **Vector Search (Optional):** FAISS  

---

## ğŸ“‚ Project Structure
Multimodal-Semantic-Retrieval/
â”‚
â”œâ”€â”€ data/
â”‚ â””â”€â”€ images/
â”œâ”€â”€ embeddings/
â”‚ â””â”€â”€ image_embeddings.npy
â”œâ”€â”€ clip_model.py
â”œâ”€â”€ search.py
â”œâ”€â”€ app.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore

---

## â–¶ï¸ Installation & Setup

### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/Esha-21/Multimodal-Semantic-Retrieval.git
cd Multimodal-Semantic-Retrieval

2ï¸âƒ£ Install dependencies
pip install -r requirements.txt

3ï¸âƒ£ Run the application
streamlit run app.py
"# -Multimodal-Semantic-Retrieval"  git init
"# -Multimodal-Semantic-Retrieval" 
